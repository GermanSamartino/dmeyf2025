---
title: "TP2-Entrega"
author: "Germán Samartino Salerno"
date: "02/10/2025"
output:
   html_document:
     toc: yes
     code_folding: show
     toc_float: yes
     df_print: paged
     theme: united
     code_download: true
---
# START
```{r librerias, include=FALSE}
# limpio la memoria
rm(list=ls(all.names=TRUE)) # remove all objects
gc(full=TRUE, verbose=FALSE) # garbage collection


# cargo las librerias que necesito
require("data.table")
require("parallel")

if(!require("R.utils")) install.packages("R.utils")
require("R.utils")

if( !require("primes") ) install.packages("primes")
require("primes")

if( !require("utils") ) install.packages("utils")
require("utils")

if( !require("rlist") ) install.packages("rlist")
require("rlist")

if( !require("yaml")) install.packages("yaml")
require("yaml")

if( !require("lightgbm") ) install.packages("lightgbm")
require("lightgbm")

if( !require("DiceKriging") ) install.packages("DiceKriging")
require("DiceKriging")

if( !require("mlrMBO") ) install.packages("mlrMBO")
require("mlrMBO")

if( !require("zlightgbm") ) install.packages("https://storage.googleapis.com/open-courses/dmeyf2025-e4a2/zlightgbm_4.6.0.99.tar.gz", repos= NULL, type= "source")
require("zlightgbm")

library(data.table)
library(ggplot2)
library(tidyr)
library(dplyr)

Sys.time()
```



```{r read_data}
t_inicio <- proc.time()


# leo el dataset
#dataset <- fread("competencia_02_crudo.csv" )

PARAM <- list()
PARAM$experimento <- "ex-002"
PARAM$semilla_primigenia <- 254881

plocal <- list()

# 501
plocal$qcanaritos <- 5L
plocal$min_data_in_leaf <- 20L
plocal$learning_rate <- 1.0
plocal$gradient_bound <- 0.1


plocal$APO <- 5
plocal$ksemillerio <- 1

```


# Leo Dataset

```{r leo_dataset}
Sys.time()
#dataset <- fread("gs://ger_samartino_bukito3/datasets/competencia_gan_02.csv.gz")

# datasets/competencia_gan_02.csv.gz
# https://storage.googleapis.com/ger_samartino_bukito3/datasets/competencia_gan_02.csv.gz
# gs://ger_samartino_bukito3/datasets/competencia_gan_02.csv.gz


# 1. Definir la ruta local de tu proyecto actual
# (Tu getwd() actual es la ruta del proyecto)
#ruta_proyecto_actual <- "/home/ger_samartino/buckets/b1/exp/e2t2-zGBM-APO" 

# 2. Definir la ruta local donde está el archivo en el bucket montado
# El bucket es 'ger_samartino_bukito3'
# La ruta del archivo es 'datasets/competencia_gan_02.csv.gz'

# Nota: La estructura exacta de la ruta montada puede variar.
# Asumo que se monta en un subdirectorio con el nombre del bucket o un ID.
# La ruta local DEBE ser la ruta de Linux que corresponde al archivo en el bucket.
# Intentaremos la ruta directa, asumiendo que el bucket se monta en /home/ger_samartino/buckets/
ruta_archivo_local <- "/home/gustavoaldana838/buckets/b1/datasets/competencia_02_crudo.csv.gz"

# --- Proceso de setwd() Temporal ---

# 3. Guardar el directorio de trabajo actual para poder volver
dir_original <- getwd()

# 4. Intentar movernos al directorio del archivo para simplificar el fread()
# Intentaremos movernos a 'datasets'
dir_archivo <- dirname(ruta_archivo_local)
nombre_archivo_solo <- basename(ruta_archivo_local)


cat("Directorio original:", dir_original, "\n")
cat("Intentando setwd() a:", dir_archivo, "\n")


tryCatch({
  
  # A. Cambiar temporalmente el directorio de trabajo
  setwd(dir_archivo)
  
  # B. Leer el archivo con fread() usando la ruta relativa o solo el nombre
  cat("Leyendo el archivo:", nombre_archivo_solo, "\n")
  dataset <- fread(
    input = nombre_archivo_solo, 
    showProgress = TRUE
  )
  
  cat("✅ Archivo cargado exitosamente desde la ruta local montada.\n")
  
}, error = function(e) {
  # Si falla, imprimir el error y asegurar que volvemos al directorio original
  cat("❌ ERROR durante la lectura del archivo:\n")
  cat("Mensaje de error:", conditionMessage(e), "\n")
  
}, finally = {
  # C. Volver al directorio de trabajo original (¡CRUCIAL!)
  setwd(dir_original)
  cat("Directorio de trabajo restablecido a:", getwd(), "\n")
})

# Puedes verificar las dimensiones del dataset cargado aquí
if (exists("dataset")) {
    print(paste("Dimensiones del dataset:", nrow(dataset), "filas,", ncol(dataset), "columnas."))
}


Sys.time()
```

```{r clase_ternaria}
# calculo el periodo0 consecutivo
dsimple <- dataset[, list(
    "pos" = .I,
    numero_de_cliente,
    periodo0 = as.integer(foto_mes/100)*12 +  foto_mes%%100 ) ]


# ordeno
setorder( dsimple, numero_de_cliente, periodo0 )

# calculo topes
periodo_ultimo <- dsimple[, max(periodo0) ]
periodo_anteultimo <- periodo_ultimo - 1


# calculo los leads de orden 1 y 2
dsimple[, c("periodo1", "periodo2") :=
    shift(periodo0, n=1:2, fill=NA, type="lead"),  numero_de_cliente ]

# assign most common class values = "CONTINUA"
dsimple[ periodo0 < periodo_anteultimo, clase_ternaria := "CONTINUA" ]

# calculo BAJA+1
dsimple[ periodo0 < periodo_ultimo &
    ( is.na(periodo1) | periodo0 + 1 < periodo1 ),
    clase_ternaria := "BAJA+1" ]

# calculo BAJA+2
dsimple[ periodo0 < periodo_anteultimo & (periodo0+1 == periodo1 )
    & ( is.na(periodo2) | periodo0 + 2 < periodo2 ),
    clase_ternaria := "BAJA+2" ]


# pego el resultado en el dataset original y grabo
setorder( dsimple, pos )
dataset[, clase_ternaria := dsimple$clase_ternaria ]


# conteo_registros <- dataset[, .N, by = .(foto_mes, clase_ternaria)]
# 
# # Para ver el resultado ordenado por mes y por la cantidad (opcional)
# setorder(conteo_registros, foto_mes, -N)
# 
# print(conteo_registros)
# fwrite( dataset,
#     file =  "competencia_02.csv.gz",
#     sep = ","
# )
```


# TRANSFORMACIONES

```{r meses_rotos, eval=FALSE}

# --- 1. Definir el Rango de Meses ---

# Función para generar la secuencia de meses (YYYYMM)
generar_rango_meses <- function(start_mes, end_mes) {
  start_year <- start_mes %/% 100
  start_month <- start_mes %% 100
  end_year <- end_mes %/% 100
  end_month <- end_mes %% 100
  
  meses <- numeric()
  current_year <- start_year
  current_month <- start_month
  
  while (current_year < end_year || (current_year == end_year && current_month <= end_month)) {
    meses <- c(meses, current_year * 100 + current_month)
    current_month <- current_month + 1
    if (current_month > 12) {
      current_month <- 1
      current_year <- current_year + 1
    }
  }
  return(meses)
}

MESES_A_ANALIZAR <- generar_rango_meses(201901, 202110)

# --- 2. Preparación de Columnas y Resultados ---

# Excluir columnas que nunca deben ser verificadas (IDs, meses, etiquetas, etc.)
columnas_excluidas <- c(
  "numero_de_cliente", "foto_mes", "clase_ternaria", 
  "clase01", "azar", "training"
)

# Columnas numéricas que se analizarán
columnas_a_verificar <- setdiff(names(dataset), columnas_excluidas)

# **NUEVO:** Data.table para almacenar los resultados de la transformación
resultados_transformados <- data.table(foto_mes = integer(), columna = character())

cat("Iniciando análisis y conversión de 0s a NAs para", length(MESES_A_ANALIZAR), "meses...\n")

# --- 3. Aplicar la Verificación y Conversión a NA en Bucle ---

for (mes_actual in MESES_A_ANALIZAR) {
  
  # 3.1. Obtener los datos del mes actual para la verificación
  temp_dt <- dataset[foto_mes == mes_actual, .SD, .SDcols = columnas_a_verificar]
  
  if (nrow(temp_dt) > 0) {
    
    # 3.2. Verificar qué columnas cumplen la condición de 100% de ceros
    columnas_cero_logico <- temp_dt[, 
      # Condición CLAVE: Verifica que todos los valores NO-NA sean cero Y que haya datos (no todo sea NA)
      lapply(.SD, function(x) all(x == 0, na.rm = TRUE) && !all(is.na(x)))
    ]
    
    # 3.3. Extraer los nombres de las columnas que cumplen la condición
    nombres_columnas_cero <- names(columnas_cero_logico)[unlist(columnas_cero_logico)]
    
    # 3.4. Si se encuentran columnas, ¡convertir los 0 a NA y registrar!
    if (length(nombres_columnas_cero) > 0) {
      
      # **Registro de Resultados**
      nuevos_registros <- data.table(
        foto_mes = mes_actual,
        columna = nombres_columnas_cero
      )
      resultados_transformados <- rbindlist(list(resultados_transformados, nuevos_registros))
      
      # Conversión a NA en el dataset original
      for (col_name in nombres_columnas_cero) {
        dataset[foto_mes == mes_actual & get(col_name) == 0, (col_name) := NA_real_]
      }
      
      cat(sprintf("✅ Mes %d: %d columnas con 100%% de ceros convertidas a NA y registradas.\n", 
                  mes_actual, length(nombres_columnas_cero)))
    } 
  }
}

# --- 4. IMPRIMIR RESULTADOS FINALES ---

cat("\n==================================================\n")
cat("RESULTADOS FINALES: Columnas (y Mes) transformadas de 100% CERO a NA\n")

if (nrow(resultados_transformados) > 0) {
    print(resultados_transformados)
} else {
    cat("No se encontraron columnas que cumplieran la condición en ningún mes del rango.\n")
}
```


```{r delete_cols, eval=FALSE}
# Eliminar las dos columnas del dataset
dataset[, c("mprestamos_personales", "cprestamos_personales") := NULL]
```


```{r paso_Master_al_final, eval=FALSE}
# 1. Identificar las columnas que comienzan con "Master_"
columnas_a_renombrar <- grep("^Master_", names(dataset), value = TRUE)

# 2. Crear los nuevos nombres de columna
# La expresión regular busca 'Master_' al inicio, captura el resto (.*), 
# y luego reestructura el nombre poniendo lo capturado seguido de '_Master'
nuevos_nombres <- gsub("^(Master_)(.*)$", "\\2_Master", columnas_a_renombrar)

# 3. Aplicar el renombramiento
setnames(dataset, columnas_a_renombrar, nuevos_nombres)

cat("Se han renombrado las siguientes columnas:\n")
print(data.table(Original = columnas_a_renombrar, Nuevo = nuevos_nombres))
```



```{r moneda_constante, eval=FALSE}
# Tabla de factores de ajuste de inflación
# La base de ajuste es 201901 (factor = 1.00)
# Los meses anteriores tienen factores mayores a 1.00


dt_ajuste <- data.table(
  foto_mes = 201901:202108,
  factor_ajuste = c(
    1.0000,  # Factor para 201901 (Base)
    1.0678,  # Factor para 201902
    1.1178,  # Factor para 201903
    1.1563,  # Factor para 201904
    1.1917,  # Factor para 201905
    1.2240,  # Factor para 201906
    1.2509,  # Factor para 201907
    1.3004,  # Factor para 201908
    1.3770,  # Factor para 201909
    1.4223,  # Factor para 201910
    1.4828,  # Factor para 201911
    1.5383,  # Factor para 201912
    1.5730,  # Factor para 202001
    1.6047,  # Factor para 202002
    1.6583,  # Factor para 202003
    1.6831,  # Factor para 202004
    1.7091,  # Factor para 202005
    1.7474,  # Factor para 202006
    1.7812,  # Factor para 202007
    1.8293,  # Factor para 202008
    1.8812,  # Factor para 202009
    1.9520,  # Factor para 202010
    2.0136,  # Factor para 202011
    2.0943,  # Factor para 202012
    2.1791,  # Factor para 202101
    2.2570,  # Factor para 202102
    2.3656,  # Factor para 202103
    2.4621,  # Factor para 202104
    2.5439,  # Factor para 202105
    2.6246,  # Factor para 202106
    2.7033,  # Factor para 202107
    2.7700  # Factor para 202108
  )
)

# 1. Identificar las columnas que comienzan con "m"
columnas_monto <- grep("^m", names(dataset), value = TRUE)

# 2. Unir el dataset con la tabla de ajuste
# Esto añade la columna 'factor_ajuste' al dataset
setkey(dt_ajuste, foto_mes)
setkey(dataset, foto_mes)

dataset_ajustado <- dt_ajuste[dataset, nomatch = 0] # Realiza el join

# 3. Aplicar la corrección por inflación a todas las columnas "m"

# La función se aplica a todas las columnas en .SDcols
# El factor_ajuste está disponible porque se unió en el paso anterior
dataset_ajustado[, 
  (columnas_monto) := lapply(.SD, function(x) x / factor_ajuste), 
  .SDcols = columnas_monto]

# 4. Eliminar la columna de factor_ajuste si ya no se necesita
dataset_ajustado[, factor_ajuste := NULL]

# Opcional: Reemplaza tu dataset original con la versión ajustada
dataset <- dataset_ajustado
```




```{r aguinaldo, eval=FALSE}

# Asegúrate de que tu dataset esté ordenado por cliente y tiempo antes del bucle
setorder(dataset, numero_de_cliente, foto_mes) 
# (Asumiendo que esta línea ya se ejecutó previamente)

# ---
# Definir Parámetros
# ---

# **NUEVA DEFINICIÓN PARA EL BUCLE:**
MESES_AGUINALDO <- c(201906, 201907, 201912, 202001, 202006, 202007, 202012, 202101, 202106)

UMBRAL_MIN_AUMENTO <- 1.40 # 40% de aumento
UMBRAL_MAX_AUMENTO <- 1.60 # 60% de aumento
FACTOR_REDUCCION <- 0.66  # Reducir el valor del mes al 66%

# Columnas a corregir
columnas_a_corregir <- c("mpayroll", "mpayroll2")


# ---
# Aplicar la Corrección (Clipping)
# ---

# Bucle 1: Iterar sobre cada mes de aguinaldo (el nuevo bucle)
for (mes_actual in MESES_AGUINALDO) {
  

  # Bucle 2: Iterar sobre cada columna de salario
  for (col_original in columnas_a_corregir) {
    
    col_lag <- paste0(col_original, "_lag_temp") # Nombre temporal para la columna lag
    
    cat("--- Procesando columna:", col_original, "---\n")
    
    # 1. CREAR la columna lag temporal con shift()
    # NOTA: Este paso podría moverse fuera de ambos bucles si la columna de lag ya existe
    # o si se define una vez para t-1. Pero mantenerlo aquí asegura que se recalcula
    # si el dataset se modifica fuera de este código.
    dataset[, (col_lag) := shift(get(col_original), n = 1, fill = NA, type = "lag"), 
            by = numero_de_cliente]
    
    # 2. Aplicar la corrección (Clipping)
    dataset[
      # 1. Filtro por el mes objetivo (MES_AGUINALDO pasa a ser mes_actual)
      foto_mes == mes_actual &
      
      # 2. El valor del mes anterior NO debe ser cero y no debe ser NA
      get(col_lag) > 0 & !is.na(get(col_lag)) &
      
      # 3. Detectar el aumento (Ratio entre 1.40 y 1.60)
      get(col_original) / get(col_lag) >= UMBRAL_MIN_AUMENTO &
      get(col_original) / get(col_lag) <= UMBRAL_MAX_AUMENTO,
      
      # 4. Acción: Reducir el valor de la columna original al 66%
      (col_original) := get(col_original) * FACTOR_REDUCCION
    ]
    
    cat("Corrección aplicada a:", col_original, "en mes", mes_actual, "\n")
    
    # 3. ELIMINAR la columna lag temporal
    dataset[, (col_lag) := NULL]
    
    cat("Columna temporal", col_lag, "eliminada.\n\n")
  }
}


```




```{r grafico_payroll, eval=FALSE}


# 1. Definir los parámetros de tiempo y columnas
MESES_FILTRO <- 201901:202108
COLUMNAS_INTERES <- c("mpayroll", "mpayroll2")

# 2. Preparar los datos
plot_data <- dataset %>%
  filter(foto_mes %in% MESES_FILTRO) %>%
  select(foto_mes, all_of(COLUMNAS_INTERES)) %>%
  group_by(foto_mes) %>%
  summarise(
    mpayroll = mean(mpayroll, na.rm = TRUE),
    mpayroll2 = mean(mpayroll2, na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  pivot_longer(
    cols = all_of(COLUMNAS_INTERES),
    names_to = "Variable",
    values_to = "Media"
  )

# 3. Generar el gráfico de líneas con etiquetas
p <- ggplot(plot_data, aes(x = as.factor(foto_mes), y = Media, group = Variable, color = Variable)) +
  geom_line(linewidth = 1) +
  geom_point(size = 3) +
  
  # === CAPA AÑADIDA: geom_text() ===
  geom_text(
    aes(label = round(Media, 0)), # Especifica la columna Media, redondeada sin decimales
    vjust = -0,                # Mueve la etiqueta ligeramente por encima del punto
    size = 4,                    # Tamaño de la fuente
    show.legend = FALSE          # No mostrar esta capa en la leyenda
  ) +
  # ===
  
  # Configurar etiquetas y títulos
  labs(
    title = "Evolución Mensual de Pagos de Salarios (202101 - 202106)",
    x = "Mes",
    y = "Media del Salario Percibido",
    color = "Columna"
  ) +
  
  # Personalizar la estética
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    legend.position = "bottom"
  )

# Mostrar el gráfico
print(p)

```

# FE-intra mes

```{r fe_intra_mes, eval=FALSE}
# el mes 1,2, ..12 , podria servir para detectar estacionalidad
dataset[, kmes := foto_mes %% 100]

# creo un ctr_quarter que tenga en cuenta cuando
# los clientes hace 3 menos meses que estan
# ya que seria injusto considerar las transacciones medidas en menor tiempo
dataset[, ctrx_quarter_normalizado := as.numeric(ctrx_quarter) ]
dataset[cliente_antiguedad == 1, ctrx_quarter_normalizado := ctrx_quarter * 5.0]
dataset[cliente_antiguedad == 2, ctrx_quarter_normalizado := ctrx_quarter * 2.0]
dataset[cliente_antiguedad == 3, ctrx_quarter_normalizado := ctrx_quarter * 1.2]

# variable extraida de una tesis de maestria de Irlanda, se perdió el link
dataset[, mpayroll_sobre_edad := mpayroll / cliente_edad]

# ctrx_sobre_edad
dataset[, ctrx_quarter_sobre_edad := ctrx_quarter / cliente_edad]

```


# FE-historico

```{r min_max_mean, eval=FALSE}
if( !require("Rcpp")) install.packages("Rcpp", repos = "http://cran.us.r-project.org")
require("Rcpp")

# se calculan para los 6 meses previos el minimo, maximo y
# tendencia calculada con cuadrados minimos (Least Squares Regression)
cppFunction("NumericVector fhistC(NumericVector pcolumna, IntegerVector pdesde )
{
  /* Aqui se cargan los valores para la regresion */
  double  x[100] ;
  double  y[100] ;

  int n = pcolumna.size();
  // Se crea un vector de salida que contendrá 5 resultados (Tendencia, Min, Max, Avg, Lag)
  NumericVector out( 5*n );

  for(int i = 0; i < n; i++)
  {
    // 1. Calcular el LAG (t-1)
    if( pdesde[i]-1 < i )  out[ i + 4*n ]  =  pcolumna[i-1] ;
    else                     out[ i + 4*n ]  =  NA_REAL ;

    
    // 2. Cargar los valores no-NA para la ventana de regresión
    int  libre   = 0 ;
    int  xvalor  = 1 ;

    for( int j= pdesde[i]-1;  j<=i; j++ )
    {
        double a = pcolumna[j] ;

        if( !R_IsNA( a ) )
        {
          y[ libre ]= a ;
          x[ libre ]= xvalor ;
          libre++ ;
        }

        xvalor++ ;
    }

    /* 3. Realizar los cálculos (Trend, Min, Max, Avg) si hay al menos dos valores */
    if( libre > 1 )
    {
      // Inicializar sumas y min/max con el primer punto (h=0)
      double  xsum  = x[0] ;
      double  ysum  = y[0] ;
      double  xysum = x[0] * y[0] ;
      double  xxsum = x[0] * x[0] ;
      double  vmin  = y[0] ;
      double  vmax  = y[0] ;

      // Acumular sumas y buscar min/max en el resto de los puntos (h=1 hasta libre-1)
      for( int h=1; h < libre; h++ ) // <-- CORRECCIÓN: Sintaxis correcta del for
      {
          xsum  += x[h] ;            // <-- CORRECCIÓN: Sumatoria de x
          ysum  += y[h] ;            // <-- CORRECCIÓN: Sumatoria de y
          xysum += x[h] * y[h] ;     // <-- CORRECCIÓN: Sumatoria de x*y
          xxsum += x[h] * x[h] ;     // <-- CORRECCIÓN: Sumatoria de x*x
          
          if( y[h] < vmin )  vmin = y[h] ; // Min
          if( y[h] > vmax )  vmax = y[h] ; // Max
      }
      
      // Asignar la Tendencia (m = (N*Sigma_xy - Sigma_x*Sigma_y) / (N*Sigma_xx - Sigma_x^2))
      out[ i ]      =  (libre*xysum - xsum*ysum)/(libre*xxsum -xsum*xsum) ;
      out[ i + n ]    =  vmin ;
      out[ i + 2*n ]  =  vmax ;
      out[ i + 3*n ]  =  ysum / libre ; // Promedio (Average)
    }
    else
    {
      // Asignar NA si no hay suficientes datos
      out[ i       ]  =  NA_REAL ;
      out[ i + n   ]  =  NA_REAL ;
      out[ i + 2*n ]  =  NA_REAL ;
      out[ i + 3*n ]  =  NA_REAL ;
    }
  }

  return  out;
}")
```



```{r min_max_mean2, eval=FALSE}
# calcula la tendencia de las variables cols de los ultimos 6 meses
# la tendencia es la pendiente de la recta que ajusta por cuadrados minimos
# La funcionalidad de ratioavg es autoria de  Daiana Sparta,  UAustral  2021

TendenciaYmuchomas <- function(
    dataset, cols, ventana = 6, tendencia = TRUE,
    minimo = TRUE, maximo = TRUE, promedio = TRUE,
    ratioavg = FALSE, ratiomax = FALSE) {
  gc(verbose= FALSE)
  # Esta es la cantidad de meses que utilizo para la historia
  ventana_regresion <- ventana

  last <- nrow(dataset)

  # creo el vector_desde que indica cada ventana
  # de esta forma se acelera el procesamiento ya que lo hago una sola vez
  vector_ids <- dataset[ , numero_de_cliente ]

  vector_desde <- seq(
    -ventana_regresion + 2,
    nrow(dataset) - ventana_regresion + 1
  )

  vector_desde[1:ventana_regresion] <- 1

  for (i in 2:last) {
    if (vector_ids[i - 1] != vector_ids[i]) {
      vector_desde[i] <- i
    }
  }
  for (i in 2:last) {
    if (vector_desde[i] < vector_desde[i - 1]) {
      vector_desde[i] <- vector_desde[i - 1]
    }
  }

  for (campo in cols) {
    nueva_col <- fhistC(dataset[, get(campo)], vector_desde)

    if (tendencia) {
      dataset[, paste0(campo, "_tend", ventana) :=
        nueva_col[(0 * last + 1):(1 * last)]]
    }

    if (minimo) {
      dataset[, paste0(campo, "_min", ventana) :=
        nueva_col[(1 * last + 1):(2 * last)]]
    }

    if (maximo) {
      dataset[, paste0(campo, "_max", ventana) :=
        nueva_col[(2 * last + 1):(3 * last)]]
    }

    if (promedio) {
      dataset[, paste0(campo, "_avg", ventana) :=
        nueva_col[(3 * last + 1):(4 * last)]]
    }

    if (ratioavg) {
      dataset[, paste0(campo, "_ratioavg", ventana) :=
        get(campo) / nueva_col[(3 * last + 1):(4 * last)]]
    }

    if (ratiomax) {
      dataset[, paste0(campo, "_ratiomax", ventana) :=
        get(campo) / nueva_col[(2 * last + 1):(3 * last)]]
    }
  }
}

```

```{r lags}
# Feature Engineering Historico
# Creacion de LAGs
setorder(dataset, numero_de_cliente, foto_mes)

# todo es lagueable, menos la primary key y la clase
cols_lagueables <- copy( setdiff(
  colnames(dataset),
  c("numero_de_cliente", "foto_mes", "clase_ternaria")
))

# https://rdrr.io/cran/data.table/man/shift.html

# lags de orden 1
dataset[,
  paste0(cols_lagueables, "_lag1") := shift(.SD, 1, NA, "lag"),
  by= numero_de_cliente,
  .SDcols= cols_lagueables
]

# lags de orden 2
dataset[,
  paste0(cols_lagueables, "_lag2") := shift(.SD, 2, NA, "lag"),
  by= numero_de_cliente,
  .SDcols= cols_lagueables
]

# agrego los delta lags
for (vcol in cols_lagueables)
{
  dataset[, paste0(vcol, "_delta1") := get(vcol) - get(paste0(vcol, "_lag1"))]
  dataset[, paste0(vcol, "_delta2") := get(vcol) - get(paste0(vcol, "_lag2"))]
}

Sys.time()
```



```{r fe_hist_param, eval=FALSE}
# parametros de Feature Engineering Historico de Tendencias
PARAM$FE_hist$Tendencias$run <- TRUE
PARAM$FE_hist$Tendencias$ventana <- 6
PARAM$FE_hist$Tendencias$tendencia <- TRUE
PARAM$FE_hist$Tendencias$minimo <- TRUE
PARAM$FE_hist$Tendencias$maximo <- TRUE
PARAM$FE_hist$Tendencias$promedio <- TRUE
PARAM$FE_hist$Tendencias$ratioavg <- FALSE
PARAM$FE_hist$Tendencias$ratiomax <- FALSE

# aqui se agregan las tendencias de los ultimos 6 meses

cols_lagueables <- intersect(cols_lagueables, colnames(dataset))
setorder(dataset, numero_de_cliente, foto_mes)

if( PARAM$FE_hist$Tendencias$run) {
    TendenciaYmuchomas(dataset,
    cols = cols_lagueables,
    ventana = PARAM$FE_hist$Tendencias$ventana, # 6 meses de historia
    tendencia = PARAM$FE_hist$Tendencias$tendencia,
    minimo = PARAM$FE_hist$Tendencias$minimo,
    maximo = PARAM$FE_hist$Tendencias$maximo,
    promedio = PARAM$FE_hist$Tendencias$promedio,
    ratioavg = PARAM$FE_hist$Tendencias$ratioavg,
    ratiomax = PARAM$FE_hist$Tendencias$ratiomax
  )
}

ncol(dataset)
Sys.time()

```

```{r FE_RF, eval=FALSE}
AgregaVarRandomForest <- function() {

  cat( "inicio AgregaVarRandomForest()\n")
  gc(verbose= FALSE)
  dataset[, clase01 := 0L ]
  dataset[ clase_ternaria %in% c( "BAJA+2", "BAJA+1"),
      clase01 := 1L ]

  campos_buenos <- setdiff(
    colnames(dataset),
    c( "clase_ternaria", "clase01")
  )

  dataset[, entrenamiento :=
    as.integer( foto_mes %in% PARAM$FE_rf$train$training )]

  dtrain <- lgb.Dataset(
    data = data.matrix(dataset[entrenamiento == TRUE, campos_buenos, with = FALSE]),
    label = dataset[entrenamiento == TRUE, clase01],
    free_raw_data = FALSE
  )

  modelo <- lgb.train(
     data = dtrain,
     param = PARAM$FE_rf$lgb_param,
     verbose = -100
  )

  cat( "Fin construccion RandomForest\n" )
  # grabo el modelo, achivo .model
  lgb.save(modelo, file="modelo.model" )

  qarbolitos <- copy(PARAM$FE_rf$lgb_param$num_iterations)

  periodos <- dataset[ , unique( foto_mes ) ]

  for( periodo in  periodos )
  {
    cat( "periodo = ", periodo, "\n" )
    datamatrix <- data.matrix(dataset[ foto_mes== periodo, campos_buenos, with = FALSE])

    cat( "Inicio prediccion\n" )
    prediccion <- predict(
        modelo,
        datamatrix,
        type = "leaf"
    )
    cat( "Fin prediccion\n" )

    for( arbolito in 1:qarbolitos )
    {
       cat( arbolito, " " )
       hojas_arbol <- unique(prediccion[ , arbolito])

       for (pos in 1:length(hojas_arbol)) {
         # el numero de nodo de la hoja, estan salteados
         nodo_id <- hojas_arbol[pos]
         dataset[ foto_mes== periodo, paste0(
            "rf_", sprintf("%03d", arbolito),
             "_", sprintf("%03d", nodo_id)
          ) :=  as.integer( nodo_id == prediccion[ , arbolito]) ]

       }

       rm( hojas_arbol )
    }
    cat( "\n" )

    rm( prediccion )
    rm( datamatrix )
    gc(verbose= FALSE)
  }

  gc(verbose= FALSE)

  # borro clase01 , no debe ensuciar el dataset
  dataset[ , clase01 := NULL ]

}
```


```{r FE_RF2, eval=FALSE}
# Parametros de Feature Engineering  a partir de hojas de Random Forest

# Estos CUATRO parametros son los que se deben modificar
PARAM$FE_rf$arbolitos= 20
PARAM$FE_rf$hojas_por_arbol= 16
PARAM$FE_rf$datos_por_hoja= 100
PARAM$FE_rf$mtry_ratio= 0.2

# Estos son quasi fijos
PARAM$FE_rf$train$training <- c( 202101, 202102, 202103)

# Estos TAMBIEN son quasi fijos
PARAM$FE_rf$lgb_param <-list(
    # parametros que se pueden cambiar
    num_iterations = PARAM$FE_rf$arbolitos,
    num_leaves  = PARAM$FE_rf$hojas_por_arbol,
    min_data_in_leaf = PARAM$FE_rf$datos_por_hoja,
    feature_fraction_bynode  = PARAM$FE_rf$mtry_ratio,

    # para que LightGBM emule Random Forest
    boosting = "rf",
    bagging_fraction = ( 1.0 - 1.0/exp(1.0) ),
    bagging_freq = 1.0,
    feature_fraction = 1.0,

    # genericos de LightGBM
    max_bin = 31L,
    objective = "binary",
    first_metric_only = TRUE,
    boost_from_average = TRUE,
    feature_pre_filter = FALSE,
    force_row_wise = TRUE,
    verbosity = -100,
    max_depth = -1L,
    min_gain_to_split = 0.0,
    min_sum_hessian_in_leaf = 0.001,
    lambda_l1 = 0.0,
    lambda_l2 = 0.0,

    pos_bagging_fraction = 1.0,
    neg_bagging_fraction = 1.0,
    is_unbalance = FALSE,
    scale_pos_weight = 1.0,

    drop_rate = 0.1,
    max_drop = 50,
    skip_drop = 0.5,

    extra_trees = FALSE
  )

# Feature Engineering agregando variables de Random Forest
#  aqui es donde se hace el trabajo
AgregaVarRandomForest()

ncol(dataset)
Sys.time()


```



# Guardo Dataset

```{r guardo_dataset}
fwrite( dataset,
    file =  "competencia_exp_02.csv.gz",
    sep = ","
)

Sys.time()
```




# Modelado


```{r opt_defino_train}

PARAM$train_final$future <- c(202106)

PARAM$train_final$meses <- c(
  202009, 202010, 202011, 202012,
  202101, 202102, 202103, 202104
)

PARAM$train_final$undersampling <- 0.01

```



# Final Training


```{r train_final_param}
# se filtran los meses donde se entrena el modelo final
dataset_train_final <- dataset[foto_mes %in% PARAM$train_final$meses]

# canaritos
PARAM$train_final$lgbm$qcanaritos <- plocal$qcanaritos

cols0 <- copy(colnames(dataset_train_final))
filas <- nrow(dataset_train_final)

if( PARAM$train_final$lgbm$qcanaritos > 0 ) {
  for( i in seq(PARAM$train_final$lgbm$qcanaritos) ){
    dataset_train_final[, paste0("canarito_",i) := runif( filas) ]
  }

  # las columnas canaritos mandatoriamente van al comienzo del dataset
  cols_canaritos <- copy( setdiff( colnames(dataset_train_final), cols0 ) )
  setcolorder( dataset_train_final, c( cols_canaritos, cols0 ) )
}

Sys.time()





# Undersampling, van todos los "BAJA+1" y "BAJA+2" y solo algunos "CONTINIA"

set.seed(PARAM$semilla_primigenia, kind = "L'Ecuyer-CMRG")
dataset_train_final[, azar := runif(nrow(dataset_train_final))]
dataset_train_final[, training := 0L]

# Target Engineering

dataset_train_final[
  (azar <= PARAM$train_final$undersampling | clase_ternaria %in% c("BAJA+1", "BAJA+2")),
  training := 1L
]

dataset_train_final[, azar:= NULL] # elimino la columna azar

# paso la clase a binaria que tome valores {0,1}  enteros
#  BAJA+1 y BAJA+2  son  1,   CONTINUA es 0
#  a partir de ahora ya NO puedo cortar  por prob(BAJA+2) > 1/40

dataset_train_final[,
  clase01 := ifelse(clase_ternaria %in% c("BAJA+2","BAJA+1"), 1L, 0L)
]


```


# Final Model

```{r final_model}
PARAM$train_final$lgbm$param_completo <-  list(
  boosting= "gbdt",
  objective= "binary",
  metric= "custom",
  first_metric_only= FALSE,
  boost_from_average= TRUE,
  feature_pre_filter= FALSE,
  force_row_wise= TRUE,
  verbosity= -100,

  seed= PARAM$semilla_primigenia,

  max_bin= 31L,
  min_data_in_leaf= plocal$min_data_in_leaf,  #este ya es el valor default de LightGBM

  num_iterations= 9999L, # dejo libre la cantidad de arboles, zLightGBM se detiene solo
  num_leaves= 9999L, # dejo libre la cantidad de hojas, zLightGBM sabe cuando no hacer un split
  learning_rate= plocal$learning_rate,  # se lo deja en 1.0 para que si el score esta por debajo de gradient_bound no se lo escale
    
  feature_fraction= 0.50, # un valor equilibrado, habra que probar alternativas ...
    
  canaritos= PARAM$train_final$lgbm$qcanaritos, # fundamental en zLightGBM, aqui esta el control del overfitting
  gradient_bound= plocal$gradient_bound   # default de zLightGBM
)

Sys.time()

# Semillerio Final
PARAM$train_final$APO <- plocal$APO
PARAM$train_final$ksemillerio  <- plocal$ksemillerio

PARAM$train_final$cortes <- c(8000, 8500, 9000, 9500, 10000, 10500, 11000, 11500, 12000)

if( !require("zlightgbm") ) install.packages("https://storage.googleapis.com/open-courses/dmeyf2025-e4a2/zlightgbm_4.6.0.99.tar.gz", repos= NULL, type= "source")
require("zlightgbm")

if(!require("primes")) install.packages("primes")
require("primes")

primos <- generate_primes(min = 100000, max = 1000000)
set.seed(PARAM$semilla_primigenia, kind = "L'Ecuyer-CMRG")
PARAM$train_final$semillas <- sample(primos)[seq( PARAM$train_final$APO*PARAM$train_final$ksemillerio )]
PARAM$train_final$semillas

campos_buenos <- setdiff(
  colnames(dataset_train_final),
  c( "clase_ternaria", "clase01", "training", "azar")
)

# dejo los datos en formato LightGBM
dtrain_final <- lgb.Dataset(
  data= data.matrix(dataset_train_final[training == 1L, campos_buenos, with= FALSE]),
  label= dataset_train_final[training == 1L, clase01],
  free_raw_data= FALSE
)

cat("filas", nrow(dtrain_final), "columnas", ncol(dtrain_final), "\n")
Sys.time()



```


# Training

```{r training}
# genero los modelitos
dir.create( "modelitos", showWarnings= FALSE)

param_completo <- copy( PARAM$train_final$lgbm$param_completo)

for( sem in PARAM$train_final$semillas ) {

  arch_modelo <- paste0("./modelitos/mod_", sem, ".txt")
  if( !file.exists( arch_modelo ) )
  {
    param_completo$seed <- sem

    modelito <- lgb.train(
      data= dtrain_final,
      param= param_completo
    )

    lgb.save( modelito, filename= arch_modelo)
    rm(modelito)
    gc()
  }
}

Sys.time()

```


# Scoring

```{r scoring}
dfuture <- dataset[foto_mes %in% PARAM$train_final$future ]

cols0 <- copy(colnames(dfuture))
filas <- nrow(dfuture)

if( PARAM$train_final$lgbm$qcanaritos > 0 ) {
  for( i in seq(PARAM$train_final$lgbm$qcanaritos) ){
    dfuture[, paste0("canarito_",i) := runif( filas) ]
  }

  # las columnas canaritos mandatoriamente van al comienzo del dataset
  cols_canaritos <- copy( setdiff( colnames(dfuture), cols0 ) )
  setcolorder( dfuture, c( cols_canaritos, cols0 ) )
}

# dataset de future, donde en este caso estoy haciendo testing

mfuture <- data.matrix(dfuture[, campos_buenos, with= FALSE])

dfuture[, ganancia := ifelse(clase_ternaria=="BAJA+2", 780000, -20000)]

mganancias <- matrix( nrow=PARAM$train_final$APO, ncol= length(PARAM$train_final$cortes) )

if( file.exists("prediccion.txt") )
  file.remove("prediccion.txt")

# aplico el modelo a los datos del future

for( vapo in seq(PARAM$train_final$APO) ) {
  # inicializacion en CERO
  vpred_acum <- rep(0.0, nrow(dfuture))
  qacumulados <- 0

  desde <- 1 + (vapo-1)*PARAM$train_final$ksemillerio
  hasta <- desde + PARAM$train_final$ksemillerio - 1
  semillas <- PARAM$train_final$semillas[desde:hasta]

  for( sem in semillas ) {

    arch_modelo <- paste0("./modelitos/mod_", sem, ".txt")
    if( file.exists( arch_modelo ) )
    {
      modelo_final <- lgb.load(arch_modelo) # leo del disco
      # hago el predict() y acumulo
      vpred_acum <- vpred_acum + predict(modelo_final, mfuture)
      qacumulados <- qacumulados + 1
      rm(modelo_final)
      gc()
    }
  }

  if( qacumulados > 0 ) {
    vpred_acum <- vpred_acum / qacumulados  # paso a probabildiad
    # tabla de prediccion, puede ser util para futuros ensembles
    #  ya que le modelo ganador va a ser un ensemble de LightGBMs

    tb_prediccion <- dfuture[, list(numero_de_cliente, foto_mes, ganancia)]
    tb_prediccion[, meta_modelo := vapo]
    tb_prediccion[, prob := vpred_acum ]
    setorder( tb_prediccion, -prob )
    tb_prediccion[, gan_acum := cumsum(ganancia)]
    tb_prediccion[, ganancia := NULL ]

    # acumulo las ganancias
    for( icor in seq(length(PARAM$train_final$cortes)) ){
      mganancias[ vapo, icor ] <- tb_prediccion[ PARAM$train_final$cortes[icor], gan_acum ]
    }

    # grabo las probabilidades del modelo
    fwrite(tb_prediccion,
      file= "prediccion.txt",
      sep= "\t",
      append= TRUE
    )

    rm(tb_prediccion)
    gc()
  }
}

Sys.time()

mganancias
```


```{r mgan}
write.table(
  x = mganancias,
  file = "mganancias_exportadas.txt",
  sep = "\t",
  col.names = TRUE,
  row.names = FALSE,
  quote = FALSE # Importante: evita comillas alrededor de los valores
)
```


```{r clasificacion}
# genero archivos con los  "envios" mejores
dir.create("kaggle", showWarnings=FALSE)

tb_prediccion <- fread("prediccion.txt")

# genero archivos de fantasia, que NO son el que voy a subir a la Pseudo Competencia Kaggle
envios <- 11000

for( vapo in seq(PARAM$train_final$APO) ) {
  if( tb_prediccion[meta_modelo==vapo, .N] > 0 ) {
    tb_pred <- tb_prediccion[meta_modelo==vapo]
    setorder( tb_pred, -prob )
    tb_pred[, Predicted := 0L] # seteo inicial a 0
    tb_pred[1:envios, Predicted := 1L] # marco los primeros

    archivo_kaggle <- paste0("./kaggle/KA", PARAM$experimento, "_", vapo, "_", envios, ".csv")

    # grabo el archivo
    fwrite(tb_pred[, list(numero_de_cliente, Predicted)],
      file= archivo_kaggle,
      sep= ","
    )

    rm( tb_pred )
    gc()
  }
}

Sys.time()


```

```{r subida_kaggle}
colmedias <- colMeans( mganancias, na.rm=TRUE )
mcorte_mejor <- max(colmedias, na.rm=TRUE)
icorte_mejor <- which.max( colmedias )
corte_mejor <- PARAM$train_final$cortes[icorte_mejor]

tbl <- as.data.table( as.list( colmedias ) )
colnames(tbl) <- paste0( "e", PARAM$train_final$cortes )
tbl[, experimento := PARAM$experimento ]

exp_gral <- "/content/buckets/b1/exp/apo-gral"
dir.create(exp_gral, showWarnings=FALSE)
fwrite( tbl,
  file= paste0( exp_gral, "/tb_experimentos.txt"),
  sep= "\t",
  append= TRUE
)

colnames( mganancias ) <- paste0( "e", PARAM$train_final$cortes )
tbl_local <- as.data.table( mganancias )

fwrite( tbl_local,
  file= "tb_apo.txt",
  sep= "\t"
)

icerca <- which.min(  abs( tb_prediccion$gan_acum - mcorte_mejor ) )
vmodelo <- tb_prediccion[ icerca, meta_modelo ]
tb_pred <- tb_prediccion[meta_modelo==vmodelo]

mcorte_mejor
icerca
tb_prediccion[ icerca]

icerca <- which.min(  abs( tb_pred$gan_acum - mcorte_mejor ) )
icerca

icerca <- which.min(  abs( tb_prediccion$gan_acum - mcorte_mejor ) )
vmodelo <- tb_prediccion[ icerca, meta_modelo ]
tb_pred <- tb_prediccion[meta_modelo==vmodelo]

icerca <- which.min(  abs( tb_pred$gan_acum - mcorte_mejor ) )
tb_pred[, Predicted := 0L] # seteo inicial a 0
tb_pred[1:icerca, Predicted := 1L] # marco los primeros

archivo_pseudo_kaggle <- paste0("./kaggle/KA", PARAM$experimento, "_",  icerca, ".csv")

# grabo el archivo
fwrite(tb_pred[, list(numero_de_cliente, Predicted)],
  file= archivo_pseudo_kaggle,
  sep= ","
)


# # la subida a Kaggle
# comando <- "kaggle competitions submit"
# competencia <- "-c  test-202106"
# arch <- paste( "-f", archivo_pseudo_kaggle)
# mensaje <-  paste0( "-m 'exp=", PARAM$experimento,
#   "  ", paste(names(plocal), plocal, sep= "=", collapse= ";" ),
#   " envios=", icorte_mejor,"'")
# 
#                     
# linea <- paste( comando, competencia, arch, mensaje)
# salida <- system(linea, intern=TRUE)
# cat(salida)


write.table(
  x = mganancias,
  file = "mganancias_exportadas.txt",
  sep = "\t",
  col.names = TRUE,
  row.names = FALSE,
  quote = FALSE # Importante: evita comillas alrededor de los valores
)

dt_metricas_clave <- data.table(
  mcorte_mejor = mcorte_mejor,
  icerca = icerca
)

write.table(
  x = dt_metricas_clave,
  file = "metricas.txt",
  sep = "\t",
  col.names = TRUE,
  row.names = FALSE,
  quote = FALSE # Importante: evita comillas alrededor de los valores
)


Sys.time()

# 1. Definir el tiempo final
t_fin <- proc.time()
cat("Fin del script registrado:", Sys.time(), "\n")

# 2. Calcular la duración (tiempo transcurrido)
# La diferencia se calcula directamente entre los objetos proc.time
duracion_proc <- t_fin - t_inicio 

# 3. Extraer el tiempo transcurrido (elapsed time)
duracion_total <- duracion_proc["elapsed"]

# 4. Crear un data.table para guardar las métricas
dt_tiempo <- data.table(
  metrica = c("t_inicio", "t_fin", "duracion_segundos", "duracion_minutos"),
  valor = c(
    format(Sys.time(), "%Y-%m-%d %H:%M:%S"), # Formato legible para t_inicio
    format(Sys.time(), "%Y-%m-%d %H:%M:%S"), # Formato legible para t_fin
    round(duracion_total, 2),
    round(duracion_total/60, 2)
  )
)

# 5. Guardar el data.table en formato CSV
nombre_archivo_salida <- "duracion_ejecucion.csv"

fwrite(
  dt_tiempo,
  file = nombre_archivo_salida,
  sep = ",",
  col.names = TRUE,
  row.names = FALSE
)

cat("\n✅ Registro de tiempo guardado exitosamente en:", nombre_archivo_salida, "\n")
cat(paste("Duración total del script:", round(duracion_total/60, 2), "minutos.\n"))

```